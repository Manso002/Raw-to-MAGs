# WHAT IS BINNING?
Metagenomics binning is a process used to classify DNA sequences obtained from metagenomic sequencing into discrete groups, or bins, based on their similarity to each other.

Ideally, we do not want to be creating bins from all of the assembled contigs, as there is often a long tail of contigs which are only several k-mers long. These have little biological meaning, as they are too short for robust gene annotation, and they can introduce a significant degree of noise in the clustering algorithms used for binning. We therefore identify a suitable threshold for a minimum length of contigs to be considered for binning.

Most binning tools have a default cut-off for minimum contig size - **MetaBAT** uses a default minimum of 2,500 bp, and recommends at least 1,500 bp. By contrast, **MaxBin** sets the minimum length at 1,000 bp.

## Coverage profiles - read mapping
Binning is done using a combination of information encoded in the **composition** and **coverage** of the assembled contigs.

The composition of the contigs is calculated by the binning tool at run time, but to obtain coverage information we must map our unassembled reads from each sample against the assembly to generate the differential abundance profiles for each contig. This is achieved using bowtie2/BBMap to map the reads against the assembly, then samtools to sort and compress the resulting file. Since we know how Bowtie2 works, we will work with that:

### Creating a mapping index

```bash
cd Assemblies
conda activate bowtiwe2
bowtie2-build Careful-LIP-1-contigs.fasta LIP-1_bw
bowtie2-build Careful-LIP-2-contigs.fasta LIP-2_bw
````
Since we are going to work with both samples sepparetly (de novo), I moved files from each sample to their respective folder. We could use for loops, but having 2 samples makes it not hard to code twice.

### Mapping the reads
```bash
cd LIP-1
ls 
Careful-LIP-1-contigs.fasta  Careful-LIP-1-scaffolds.fasta  LIP-1_bw.1.bt2  LIP-1_bw.2.bt2  LIP-1_bw.3.bt2  LIP-1_bw.4.bt2  LIP-1_bw.rev.1.bt2  LIP-1_bw.rev.2.bt2
````
Now that we have our index, we can map the reads.
````bash
bowtie2 --threads 16 --sensitive -x LIP-1_bw -1 ./DNA-LIP-1_clean_nohuman_R1.fastq.gz -2 ./DNA-LIP-2_clean_nohuman_R1.fastq.gz -S LIP-1.sam
````
### Sort and compress results
As for now, results are stored in sam format. This is a compact text representation of where each short read sits in the contigs. In order to save disk space and prepare the file for downstream analysis we do:
```bash
samtools sort -@ 16 -o LIP-1.bam LIP-1.sam
```
*Repeat the process with sample 2*
## Binning with MetaBAT and MaxBin 
There is a multitude of good binning tools published, each with their own strengths and weaknesses. The common approach when binning, is using a few (we will use 2) and then use **DAS_Tool** to evaluate all potential outcomes and define the best set of bins across all tools used. 

*Again, I will show how to do it with the sample 1*

- ### MetaBAT
Here, the *bam* files we just created are parsed into a tab-delimited table of the average coverage depth per sample mapped. Binning is then performed using this table

```bash
mkdir Binning
# Create symbolic links
ln -rs ./LIP-1.bam ./Binning/LIP-1/LIP-1.bam
ln -rs ./LIP-2.bam ./Binning/LIP-2/LIP-2.bam
````
Install binning tools
```bash
conda create -n binning 
conda activate binning
conda install -c bioconda metabat2 maxbin2 concoct
````
Now we use the script jgi_summarize_bam_contig_depths 
```bash
jgi_summarize_bam_contig_depths --outputDepth LIP-1-metabat.txt ./LIP-1.bam
````
Now we run metabat2
```bash
metabat2 -t 12 -m 1500 -i ./Careful-LIP-1-contigs.fasta -a LIP-1-metabat.txt -o Metabat/LIP-1-metabat
# Resulted in 30 bins
metabat2 -t 12 -m 1500 -i ./Careful-LIP-2-contigs.fasta -a LIP-2-metabat.txt -o Metabat/LIP-2-metabat
# Resulted in 29 bins
````
- ### MaxBin
Like MetaBAT, MaxBin requires a text representation of coverage information for binning. We can reformat the metabat coverage files into the format expected by MaxBin. We use cut to select only the columns of interest, which are the *contigName* and *coverage* columns:
```bash
less LIP-1-metabat.txt
cut -f1,4,6,8,10 LIP-1-metabat.txt > LIP-1-maxbin.txt
````
Run maxbin 
```bash
run_MaxBin.pl -thread 12 -min_contig_length 1500 -contig ./Careful-LIP-1-contigs.fasta -abund LIP-1-maxbin.txt -out maxbin/LIP-1-maxbin
# 29 bins
run_MaxBin.pl -thread 12 -min_contig_length 1500 -contig ./         Careful-LIP-2-contigs.fasta -abund LIP-2-maxbin.txt -out maxbin/LIP-2-maxbin
# 21 bins
````
## Bin dereplication - DAS-Tool
For the moment, we have generated four sets of bins from the assembly of two samples. We can see that metabat recovered 59 bins, while maxbin recovered 50. We have recovered a considerable ammount of bins for an isolate metagenomic sample. I assume samples were contaminated with other organisms.

It is not clear which tool has done a better job of recruiting contigs to each bin, they will likely be of differing quality. **DAS_Tool** is a program designed to analyse the bins in each of our binning sets and determine where these equivalent pairs exist and return the 'best' one. It does not use actual bins, but a set of text files we are going to produce:
### Create contig/bin tables
- ### Metabat

For each of our binning tools, we need to extract the contigs assigned to each bin and create a single file that report these as 
```bash
Contig[tab]Bin
````
```bash
# Create a LIP-1-metabat-associations.txt
for bin_path in Metabat/*.fa; do
    bin_name=$(basename ${bin_path} .fa)
    grep ">" ${bin_path} | sed 's/>//g' | sed "s/$/\t${bin_name}/g" >> LIP-1-metabat_associations.txt
done
````
- ### Maxbin
Basically same process, but we need to change file extension, as MaxBin writes output using the .fasta suffix:
```bash
for bin_path in maxbin/*.fasta;
do
    bin_name=$(basename ${bin_path} .fasta)
    grep ">" ${bin_path} | sed 's/>//g' | sed "s/$/\t${bin_name}/g" >> LIP-1-maxbin_associations.txt
done
````
## DAS_Tools bin dereplication
```bash
# we are still in binning enviroment
conda install -c bioconda das_tool
````
```bash
# LIP-1
DAS_Tool -i LIP-1-metabat_associations.txt,LIP-1-maxbin_associations.txt -c ./Careful-LIP-1-contigs.fasta -l metabat,maxbin --search_engine diamond --write_bin_evals --write_bins -t 12 -o DAS_Tool/LIP-1-Dast
````
By looking at the summary.tsv file generated by DAS_Tool, we can see the number of bins selected. How many have you got?

## Evaluating bins using *CheckM*
Now that we have our almost final set of bins, it is a great idea to estimate their completeness (how much of the genome was recovered) and contamination (how many contigs we believe have been incorrectly assigned to that bin). You can read [CheckM](https://github.com/Ecogenomics/CheckM) documentation, since it is really used and powerful. 
```bash
# Create new enviroment and install CheckM and dependencies
conda create -n checkm python=3.9
conda activate checkm
conda install -c bioconda numpy matplotlib pysam
conda install -c bioconda hmmer prodigal pplacer
pip3 install checkm-genome
conda activate checkm
````
Run checkm
```bash
checkm lineage_wf -t 28 -x fa --tab_table -f checkm-LIP-1.txt ./LIP-1-DasT_DASTool_bins/ LIP-1-checkm
````
How many MAGs did you end up with? Would you filter some of them? Why or why not?

